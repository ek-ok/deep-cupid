{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/profile_intro.csv') as f:\n",
    "    data = f.read().splitlines()\n",
    "\n",
    "text = ''.join(data)\n",
    "\n",
    "chars = sorted(set(text))\n",
    "char_to_ind = {c: i for i, c in enumerate(chars)}\n",
    "ind_to_char = {v: k for k, v in char_to_ind.items()}\n",
    "\n",
    "text_as_int = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(array, batch_shape):\n",
    "    num_samples, num_chars = batch_shape\n",
    "    total_items = num_samples * num_chars\n",
    "    num_batches = int(len(array) / total_items)\n",
    "\n",
    "    # Keep the full batches and ignore the left.\n",
    "    # from (1115394,) to (1115300,)\n",
    "    array = array[:total_items*num_batches]\n",
    "\n",
    "    # from (1115300,) to (10, 111530)\n",
    "    array = array.reshape((num_samples, -1))\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        x = array[:, i:i+num_chars]\n",
    "        y = np.roll(x, shift=-1, axis=1)\n",
    "        \n",
    "        if i >= (array.shape[1] - num_chars):\n",
    "            i = 0\n",
    "        else:\n",
    "            i += num_chars\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 loss: 4.5552\n",
      "step: 200 loss: 2.3369\n",
      "step: 400 loss: 1.8307\n",
      "step: 600 loss: 1.6458\n",
      "step: 800 loss: 1.5674\n",
      "step: 1000 loss: 1.5247\n",
      "step: 1200 loss: 1.4381\n",
      "step: 1400 loss: 1.4593\n",
      "step: 1600 loss: 1.4226\n",
      "step: 1800 loss: 1.4086\n",
      "step: 2000 loss: 1.3644\n",
      "step: 2200 loss: 1.3432\n",
      "step: 2400 loss: 1.3583\n",
      "step: 2600 loss: 1.3224\n",
      "step: 2800 loss: 1.3432\n",
      "step: 3000 loss: 1.3144\n",
      "step: 3200 loss: 1.2720\n",
      "step: 3400 loss: 1.2880\n",
      "step: 3600 loss: 1.2895\n",
      "step: 3800 loss: 1.2880\n",
      "step: 4000 loss: 1.2728\n",
      "step: 4200 loss: 1.2646\n",
      "step: 4400 loss: 1.3011\n",
      "step: 4600 loss: 1.2914\n",
      "step: 4800 loss: 1.2448\n",
      "step: 5000 loss: 1.2878\n",
      "step: 5200 loss: 1.2996\n",
      "step: 5400 loss: 1.2626\n",
      "step: 5600 loss: 1.2874\n",
      "step: 5800 loss: 1.2749\n",
      "step: 6000 loss: 1.2752\n",
      "step: 6200 loss: 1.2982\n",
      "step: 6400 loss: 1.2054\n",
      "step: 6600 loss: 1.2333\n",
      "step: 6800 loss: 1.2188\n",
      "step: 7000 loss: 1.2184\n",
      "step: 7200 loss: 1.2410\n",
      "step: 7400 loss: 1.2374\n",
      "step: 7600 loss: 1.2321\n",
      "step: 7800 loss: 1.1862\n",
      "step: 8000 loss: 1.2723\n",
      "step: 8200 loss: 1.2486\n",
      "step: 8400 loss: 1.2493\n",
      "step: 8600 loss: 1.2264\n",
      "step: 8800 loss: 1.2544\n",
      "step: 9000 loss: 1.2296\n",
      "step: 9200 loss: 1.2365\n",
      "step: 9400 loss: 1.2219\n",
      "step: 9600 loss: 1.2350\n",
      "step: 9800 loss: 1.2059\n",
      "step: 10000 loss: 1.1422\n",
      "step: 10200 loss: 1.1920\n",
      "step: 10400 loss: 1.1929\n",
      "step: 10600 loss: 1.1577\n",
      "step: 10800 loss: 1.2589\n",
      "step: 11000 loss: 1.1575\n",
      "step: 11200 loss: 1.1818\n",
      "step: 11400 loss: 1.2327\n",
      "step: 11600 loss: 1.1393\n",
      "step: 11800 loss: 1.2307\n",
      "step: 12000 loss: 1.2207\n",
      "step: 12200 loss: 1.2487\n",
      "step: 12400 loss: 1.2397\n",
      "step: 12600 loss: 1.1835\n",
      "step: 12800 loss: 1.2022\n",
      "step: 13000 loss: 1.2149\n",
      "step: 13200 loss: 1.1646\n",
      "step: 13400 loss: 1.1994\n",
      "step: 13600 loss: 1.1480\n",
      "step: 13800 loss: 1.1367\n",
      "step: 14000 loss: 1.1981\n",
      "step: 14200 loss: 1.1425\n",
      "step: 14400 loss: 1.1774\n",
      "step: 14600 loss: 1.2254\n",
      "step: 14800 loss: 1.1970\n",
      "step: 15000 loss: 1.2067\n",
      "step: 15200 loss: 1.1929\n",
      "step: 15400 loss: 1.1883\n",
      "step: 15600 loss: 1.1705\n",
      "step: 15800 loss: 1.1880\n",
      "step: 16000 loss: 1.2200\n",
      "step: 16200 loss: 1.2089\n",
      "step: 16400 loss: 1.1773\n",
      "step: 16600 loss: 1.1391\n",
      "step: 16800 loss: 1.1625\n",
      "step: 17000 loss: 1.1534\n",
      "step: 17200 loss: 1.1577\n",
      "step: 17400 loss: 1.1969\n",
      "step: 17600 loss: 1.1804\n",
      "step: 17800 loss: 1.1561\n",
      "step: 18000 loss: 1.1748\n",
      "step: 18200 loss: 1.1787\n",
      "step: 18400 loss: 1.1905\n",
      "step: 18600 loss: 1.1156\n",
      "step: 18800 loss: 1.1479\n",
      "step: 19000 loss: 1.1655\n",
      "step: 19200 loss: 1.1219\n",
      "step: 19400 loss: 1.1648\n",
      "step: 19600 loss: 1.1537\n",
      "step: 19800 loss: 1.2088\n",
      "step: 20000 loss: 1.2218\n",
      "step: 20200 loss: 1.1492\n",
      "step: 20400 loss: 1.1953\n",
      "step: 20600 loss: 1.1678\n",
      "step: 20800 loss: 1.1578\n",
      "step: 21000 loss: 1.1966\n",
      "step: 21200 loss: 1.1897\n",
      "step: 21400 loss: 1.1314\n",
      "step: 21600 loss: 1.1744\n",
      "step: 21800 loss: 1.1226\n",
      "step: 22000 loss: 1.1623\n",
      "step: 22200 loss: 1.1593\n",
      "step: 22400 loss: 1.1906\n",
      "step: 22600 loss: 1.1774\n",
      "step: 22800 loss: 1.1664\n",
      "step: 23000 loss: 1.1216\n",
      "step: 23200 loss: 1.1405\n",
      "step: 23400 loss: 1.1276\n",
      "step: 23600 loss: 1.1758\n",
      "step: 23800 loss: 1.1757\n",
      "step: 24000 loss: 1.1681\n",
      "step: 24200 loss: 1.1428\n",
      "step: 24400 loss: 1.1288\n",
      "step: 24600 loss: 1.1379\n",
      "step: 24800 loss: 1.2093\n",
      "step: 25000 loss: 1.1464\n",
      "step: 25200 loss: 1.1590\n",
      "step: 25400 loss: 1.1762\n",
      "step: 25600 loss: 1.1082\n",
      "step: 25800 loss: 1.1596\n",
      "step: 26000 loss: 1.1854\n",
      "step: 26200 loss: 1.1546\n",
      "step: 26400 loss: 1.1164\n",
      "step: 26600 loss: 1.1728\n",
      "step: 26800 loss: 1.1730\n",
      "step: 27000 loss: 1.1590\n",
      "step: 27200 loss: 1.1356\n",
      "step: 27400 loss: 1.1707\n",
      "step: 27600 loss: 1.1767\n",
      "step: 27800 loss: 1.1079\n",
      "step: 28000 loss: 1.1437\n",
      "step: 28200 loss: 1.1526\n",
      "step: 28400 loss: 1.1577\n",
      "step: 28600 loss: 1.1643\n",
      "step: 28800 loss: 1.1007\n",
      "step: 29000 loss: 1.1410\n",
      "step: 29200 loss: 1.1606\n",
      "step: 29400 loss: 1.1188\n",
      "step: 29600 loss: 1.1533\n",
      "step: 29800 loss: 1.1341\n"
     ]
    }
   ],
   "source": [
    "from char_rnn import CharRNN\n",
    "\n",
    "\n",
    "batch_shape = (100, 100)\n",
    "batches = generate_batches(text_as_int, batch_shape)\n",
    "model = CharRNN(char_to_ind, batch_shape, rnn_size=256, num_layers=3, learning_rate=0.005, grad_clip=5)\n",
    "model.train(batches, iters=30000, keep_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i29800_l256_ckpt\n",
      "I'm truthful than I do was an enversation with amount of conversation. The communication is can be silly in a cal though, and I love to go out and get old and love to travel and eat going.\"\"I've spent them almost 2 years ago, I'm a bit of an outgoing, and I talk. I like to laugh and try new things, with a book, travel, and enjoy something new about a good feaduring thing to do with. I am a consultant woman with a great lover of a good started and a great side of their food, with an adventure of all time when I'm staying in. I like to go to a show and walk or spend time one with friends.\"\"Hm there happens, there is, that takes the taste of some past few text and I like a good chean trait, tennis with friends all my friends in my face. I'm a pretty social activist and adore thinkers of the mind, and to see my backgrounds and I am a posible one, because I like to live it to the best thing. I like to think of myself as a singer. I like to take camping sports and moving the buddhish. My music,\n"
     ]
    }
   ],
   "source": [
    "from char_rnn import CharRNN\n",
    "\n",
    "model = CharRNN(char_to_ind, batch_shape, rnn_size=256, num_layers=3, learning_rate=0.005, grad_clip=5,  predict=True)\n",
    "samp = model.predict(prime=\"I'm\", num_char=1000)\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
