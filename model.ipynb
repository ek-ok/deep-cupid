{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/profile_intro.csv') as f:\n",
    "    data = f.read().splitlines()\n",
    "\n",
    "text = ''.join(data)\n",
    "\n",
    "chars = sorted(set(text))\n",
    "char_to_ind = {c: i for i, c in enumerate(chars)}\n",
    "ind_to_char = {v: k for k, v in char_to_ind.items()}\n",
    "\n",
    "text_as_int = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(array, num_samples, num_chars):\n",
    "    batch_size = num_samples * num_chars\n",
    "    num_batches = int(len(array) / batch_size)\n",
    "\n",
    "    # Keep the full batches and ignore the left.\n",
    "    # from (1115394,) to (1115300,)\n",
    "    array = array[:batch_size*num_batches]\n",
    "\n",
    "    # from (1115300,) to (10, 111530)\n",
    "    array = array.reshape((num_samples, -1))\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        x = array[:, i:i+num_chars]\n",
    "        y = np.roll(x, shift=-1, axis=1)\n",
    "        \n",
    "        if i >= (array.shape[1] - num_chars):\n",
    "            i = 0\n",
    "        else:\n",
    "            i += num_chars\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "rnn_size = 256           # Size of hidden layers in rnn_cell\n",
    "num_layers = 3           # Number of hidden layers\n",
    "learning_rate = 0.005    # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 200  loss: 3.0507  0.2654 sec/batch\n",
      "step: 400  loss: 2.2844  0.2707 sec/batch\n",
      "step: 600  loss: 1.9872  0.2803 sec/batch\n",
      "step: 800  loss: 1.8188  0.2661 sec/batch\n",
      "step: 1000  loss: 1.7130  0.2656 sec/batch\n",
      "step: 1200  loss: 1.6291  0.2726 sec/batch\n",
      "step: 1400  loss: 1.6035  0.2752 sec/batch\n",
      "step: 1600  loss: 1.5598  0.2719 sec/batch\n",
      "step: 1800  loss: 1.5149  0.2702 sec/batch\n",
      "step: 2000  loss: 1.4538  0.2744 sec/batch\n"
     ]
    }
   ],
   "source": [
    "from char_rnn import CharRNN\n",
    "\n",
    "\n",
    "model = CharRNN(len(chars), batch_size, num_steps, 'LSTM', rnn_size,\n",
    "                num_layers, learning_rate)\n",
    "batches = generate_batches(text_as_int, batch_size, num_steps)\n",
    "model.train(batches, iters=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i2000_l256_LSTM_ckpt\n",
      "Hi, I'm to me sanerion andey and atharing of mate than a day at a ban sorerond somest atemand and and the a diche. I'm saces I'mes sime soust and somatally a sime time, a lone, who ten the want mayist.. I math myseles. I'es semering my sectalall tantor and meally sountes than bean things mothiel sicite te meedingly of tite that I hund simestion myee seal serane oulestarases and to bet and a time aroris my and tat at and tous I ande the a seameder,s. \"I dore sere and meast and teer to mand tratel andine seriouse ont me mereations, tord stire semally silina that time on a thinge of the andersant to santior, tand a don ontat amporse salation to seatar and the sica as sonoother tattie to are munes or a dand the wime the sillle outhool it to the beete on astime a stele as them atal the tist saca saris at at the sean to the time in atariate of out and and sores mest tho and attertses. I love. I am the tha deast a telled and tor my and to the timple astint and somite sincoriste to soreast of soll and\n"
     ]
    }
   ],
   "source": [
    "from char_rnn import CharRNN\n",
    "\n",
    "model = CharRNN(len(chars), batch_size, num_steps, 'LSTM', rnn_size,\n",
    "                num_layers, learning_rate, sampling=True)\n",
    "\n",
    "# choose the last checkpoint and generate new text\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = model.sample(checkpoint, n_samples=1000, vocab_size=len(chars),\n",
    "                    vocab_to_ind=char_to_ind, ind_to_vocab=ind_to_char, prime=\"Hi, I'm \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
